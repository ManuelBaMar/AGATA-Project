{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se pretende crear un modelo de reduccion de la dimensionalidad basado en técnicas clasicas, más concretamente una PCA. Se pretende seguir la misma estructura que un autoencoder, es decir, generar un método de codificación de la señal para comprimir la informacion, lo cual se realizará mediante un analisis de componetes principales (PCA), y, posteriormente, analizar la capacidad de reconstrucción del modelo. \\\n",
    "En primer lugar importamos el conjunto de bibliotecas necesarias para realizar los cálculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Importamos las librerias necesarias:\n",
    "##Importamos Pandas para trabajar con DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "##Importamos Numpy para incluir mas funciones matematicas\n",
    "import numpy as np\n",
    "\n",
    "##Importamos MatPlotLib para realizar representaciones\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec #Es un paquete de matplotlib que permite realizar figuras de varias subfiguras\n",
    "\n",
    "##Importamos la funcion display de la libreria IPython.display, que permite mostrar contenido de manera mas enriquecida. Tambien importamos Image para poder mostrar gráficos\n",
    "from IPython.display import display\n",
    "\n",
    "##Importamos varias funciones de las librerias sklearn\n",
    "from sklearn.model_selection import train_test_split #Es una función que se utiliza para dividir un conjunto de datos en conjuntos de entrenamiento y prueba de forma aleatoria.\n",
    "from sklearn.decomposition import PCA #Importamos la clase PCA para poder realizare analisis de componetes principales a los datos\n",
    "\n",
    "#Importamos la librería joblib para poder exportar modelos de sklearn\n",
    "import joblib\n",
    "\n",
    "##Immportamos el generador de números aleatorios\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una serie de funciones útiles a lo largo del codigo, entre las que se encuentra: función de normalizacion min_max, funcion para convertir un datafraem en un tensor y funcion de representacion de pulsos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Función para normalizar las señales. Proporciona dos opciones: 1 para normalizar a [0,1] y 2 para normalizar respecto al valor máximo\n",
    "#Por defecto normaliza a escala [0,1]\n",
    "def normalizacion(df, tipo = 1):\n",
    "    #Normalización [0,1]\n",
    "    if tipo == 1:\n",
    "        maximos_fila = df.max(axis = 1).to_numpy() #Extraemos el valor máximo de las filas\n",
    "        minimos_fila = df.min(axis = 1).to_numpy() #Extraemos el valor mínimo de las filas\n",
    "        df = (df - minimos_fila[:, None])/(maximos_fila[:, None] - minimos_fila[:, None]) #Normalizamos los valores del Dataframe\n",
    "        return df\n",
    "    \n",
    "    #Normalización respecto al máximo\n",
    "    if tipo == 2:\n",
    "        maximos_fila = df.max(axis = 1).to_numpy() #Extraemos el valor máximos de las filas\n",
    "        df = df/maximos_fila[:, None] #Dividimos cada fila por su máximo\n",
    "        return df\n",
    "    \n",
    "##Funcion para cambiar el formato del input del autoencoder y que sea compatible con papas convolucionales 1D\n",
    "def convert_to_tensor(df):\n",
    "    #Convertimos el dataframe a un tensor\n",
    "    tensor = tf.convert_to_tensor(df, dtype = tf.float32)\n",
    "    #Añadimos la dimension de canal \n",
    "    tensor = tf.expand_dims(tensor, axis = 2)\n",
    "    return tensor\n",
    "\n",
    "##Definimos una funcion para representar de manera comoda dos señales diferentes aleatorias en un mismo grafico, que provengan del mismo conjunto de datos\n",
    "def plot_pulsos(data):\n",
    "    #Generar dos números enteros aleatorios entre 1 y el número total de muestras\n",
    "    n_random = random.randint(1, data.shape[0])\n",
    "    m_random = random.randint(1, data.shape[0])\n",
    "    \n",
    "    #Representacion de las señales en un plot\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    plt.plot(data.iloc[n_random], label = \"Señal 1\", color = \"blue\")\n",
    "    plt.plot(data.iloc[m_random], label = \"Señal 2\", color = \"red\")\n",
    "    plt.title('Señal')\n",
    "    plt.xticks([])\n",
    "    plt.ylabel('Valor')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el conjunto de datos con el que vamos a trabajar y realizamos el preprocesamiento de los datos necerario antes de aplicar el analisis de componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cargamos el conjunto de datos con el que vamos a trabajar, que en este caso son un conjunto de señales del core de AGATA\n",
    "url = \"./Data/Core_signal.dat\"\n",
    "core_signals = pd.read_csv(url, sep = \"\\t\", header = \"infer\", dtype = np.float64, chunksize = None)\n",
    "\n",
    "##Normalizamos las señales de core en [0,1] para realizar el entrenamiento del autoencoder\n",
    "core_norm = normalizacion(core_signals.copy())\n",
    "\n",
    "##Dividimos las señales del core de AGATA en un conjunto de entrenamiento, un conjunto de validación y un conjunto test, con una proporcion del 80% para el conjunto de entrenamiento\n",
    "#y un 10% para el conjunto de validación y otro 10% para el conjuto test\n",
    "X_train, X_valid = train_test_split(core_norm, train_size = 0.80)\n",
    "X_valid, X_test = train_test_split(X_valid, train_size = 0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a definir la PCA para el conjunto de entrenamiento, lo que nos va a definir una transformacion de reduccion de la dimensión que podremos aplicar al conjunto de validación y test para comprobar la valided de la transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Instanciamos la clase asociada con la PCA, definiendo los parametros que nos interesen. Al especificar el argumento n_components = Noen indicamos que se conserven todas las componetes principales\n",
    "#Al indicar el argumento svd_solver = \"covariance_eigh\" indicamos que el metodo de obtencion sea mediante la descomposicion en valores singulares de la matriz de convarianzas.\n",
    "pca = PCA(n_components = 10, svd_solver = \"covariance_eigh\")\n",
    "\n",
    "#Una vez instanciada la clase ajustamos la transformacion a partir del conjunto de entrenamiento\n",
    "pca.fit(X_train)\n",
    "\n",
    "#Guardamos en un fichero el modelo de PCA ajustado para los datos mediante la biblioteca joblib\n",
    "joblib.dump(pca, \"/home/jupyter-manuel/Digiopt/Models/PCA_model.joblib\")\n",
    "#Cargamos el modelo de PCA guardado\n",
    "pca = joblib.load(\"/home/jupyter-manuel/Digiopt/Models/PCA_model.joblib\")\n",
    "\n",
    "\n",
    "#Una vez ajustada la transformación la aplicamos al conjunto de entrenamiento, de validacion y test, con los pesos obtenidos para el conjunto de entrenamiento\n",
    "X_train_reduced = pca.transform(X_train)\n",
    "X_valid_reduced = pca.transform(X_valid)\n",
    "X_test_reduced = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluamos el error de reconstruccion dado por la PCA, manteniendo 10 componentes principales.\n",
    "X_valid_reconstructed = pca.inverse_transform(X_valid_reduced)\n",
    "\n",
    "#Calculamos el error de reconstruccion promedio dado por la PCA mediante el indicador MSE\n",
    "rmse = np.sqrt(np.mean((X_valid - X_valid_reconstructed) ** 2))\n",
    "print(f\"Error de reconstrucción (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos los resultados dados por la PCA para cada conjunto de datos a un DataFrame, para operar con ellos más adelante\n",
    "X_train_reduced_df = pd.DataFrame(X_train_reduced)\n",
    "X_valid_reduced_df = pd.DataFrame(X_valid_reduced)\n",
    "X_test_reduced_df = pd.DataFrame(X_test_reduced)\n",
    "\n",
    "#Estraemos el porcentaje de varianza explicada por cada una de las componetes principales\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "#Extraemos el autovalor correspondientre a cada componete principal\n",
    "eagenvalues = pca.singular_values_\n",
    "#Exrtaemos las coordenadas de cada componete principal en el espacio de caracteristicas original, y lo convertimos en un DataFrame\n",
    "components = pd.DataFrame(pca.components_)\n",
    "\n",
    "print(explained_variance)\n",
    "print(eagenvalues)\n",
    "display(components)\n",
    "\n",
    "# Generamos un gráfico ScreePlot para visualizar el porcentaje de varianza absorvida por cada una de las componetes principales\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Componentes Principales')\n",
    "plt.ylabel('Proporción de Varianza Explicada')\n",
    "plt.xticks(range(1, len(explained_variance) + 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
